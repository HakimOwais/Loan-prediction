from langchain.chains import LLMChain
from langchain.prompts import PromptTemplate
from langchain.chains.question_answering import load_qa_chain
from pathlib import Path
import os
import sys

PACKAGE_ROOT = Path(os.path.abspath(os.path.dirname(__file__))).parent
sys.path.append(str(PACKAGE_ROOT))
from src.components import client, llm, embeddings, TRANSACTION_COLLECTION
from chatbot.ingest import vector_store_transactions, ATLAS_VECTOR_SEARCH_INDEX_NAME

from langchain_core.prompts import PromptTemplate
from langchain_core.runnables import RunnableLambda

def answer_question_from_context(context: str, question: str):
    """
    Function to answer a question based purely on the provided context using LangChain.
    
    Args:
        context (str): The context string that provides information.
        question (str): The question to be answered based on the context.

    Returns:
        str: The answer generated by the LLM based on the context.
    """

    # Define the prompt template
    prompt_template = PromptTemplate.from_template(
        """Given the following context:
        {context}
        You are a very polite bot. You greet the user well. 
        Answer the question based purely on the context. If the information is not present in the context, say "I don't know". Please make sure your tone is professional and really structured. 

        Question: {question}
        Answer:
        """
    )

    # Convert prompt template into a runnable sequence
    chain = prompt_template | llm

    # Invoke the chain with correct inputs
    answer = chain.invoke({"context": context, "question": question})

    return answer
